Bill Wang blw002@ucsd.edu

Gal Minshe and Yusa Wang

What is the most interesting topic covered in your domain this quarter?

Neural collapse in graph transformers has stood out as an intriguing topic. It explores the convergence of neural network features into highly symmetric structures during training and raises fascinating questions about their implications for graph representation learning and model generalization.

Describe a potential investigation you would like to pursue for your Quarter 2 Project.

I want to delve into the theoretical aspects of neural collapse on graphs. This includes examining how the phenomenon manifests in graph neural networks and transformers, particularly its impact on embedding quality and downstream task performance.

What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?

I would focus on allocating more time to in-depth literature reviews and background studies to better understand the foundations of the project. This preparation could provide a stronger theoretical basis and guide the research direction.

What other techniques would you be interested in using in your project?

I am interested in exploring advanced optimization methods for training graph neural networks, such as curriculum learning and adaptive gradient clipping. Additionally, integrating spectral graph theory for feature extraction and investigating the use of unsupervised learning frameworks to improve graph representations are appealing directions. Employing explainability techniques like layer-wise relevance propagation could also offer deeper insights into model behavior.









